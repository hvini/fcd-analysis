{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import eigh\n",
    "from matplotlib import style\n",
    "import mpl_axes_aligner\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scienceplots\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(['science', 'nature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_df = pd.read_csv('../stats/preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" t1_df = t1_df\\\n",
    "    .drop(columns=t1_df.filter(like='volume_lh_pial'))\\\n",
    "    .drop(columns=t1_df.filter(like='volume_rh_pial'))\\\n",
    "    .drop(columns=t1_df.filter(like='thickness_lh_pial'))\\\n",
    "    .drop(columns=t1_df.filter(like='thickness_rh_pial'))\\\n",
    "    .drop(columns=[\"etiv_volume_lh\",\n",
    "                   \"etiv_volume_rh\",\n",
    "                   \"etiv_meancurv_lh\",\n",
    "                   \"etiv_meancurv_rh\",\n",
    "                   \"etiv_thickness_lh\",\n",
    "                   \"etiv_thickness_rh\",\n",
    "                   \"brainsegvolnotvent_volume_lh\",\n",
    "                   \"brainsegvolnotvent_volume_rh\",\n",
    "                   \"brainsegvolnotvent_thickness_lh\",\n",
    "                   \"brainsegvolnotvent_thickness_rh\",\n",
    "                   \"brainsegvolnotvent_meancurv_lh\",\n",
    "                   \"brainsegvolnotvent_meancurv_rh\",\n",
    "                   \"meanthickness_thickness_lh\",\n",
    "                   \"meanthickness_thickness_rh\"])\\\n",
    "    .drop(columns=[\"etiv_meancurv_lh_pial\", \"etiv_meancurv_rh_pial\", \"brainsegvolnotvent_meancurv_lh_pial\", \"brainsegvolnotvent_meancurv_rh_pial\"])\n",
    "t1_df.to_csv('../stats/cleaned_data.csv', index=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_a2009s_left = t1_df.filter(like='volume_lh_a2009s').assign(group=t1_df['group'])\n",
    "meancurv_a2009s_left = t1_df.filter(like='meancurv_lh_a2009s').assign(group=t1_df['group'])\n",
    "thickness_a2009s_left = t1_df.filter(like='thickness_lh_a2009s').assign(group=t1_df['group'])\n",
    "\n",
    "volume_a2009s_right = t1_df.filter(like='volume_rh_a2009s').assign(group=t1_df['group'])\n",
    "meancurv_a2009s_right = t1_df.filter(like='meancurv_rh_a2009s').assign(group=t1_df['group'])\n",
    "thickness_a2009s_right = t1_df.filter(like='thickness_rh_a2009s').assign(group=t1_df['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_pial_left = t1_df.filter(like='volume_lh_pial').assign(group=t1_df['group'])\n",
    "meancurv_pial_left = t1_df.filter(like='meancurv_lh_pial').assign(group=t1_df['group'])\n",
    "thickness_pial_left = t1_df.filter(like='thickness_lh_pial').assign(group=t1_df['group'])\n",
    "\n",
    "volume_pial_right = t1_df.filter(like='volume_rh_pial').assign(group=t1_df['group'])\n",
    "meancurv_pial_right = t1_df.filter(like='meancurv_rh_pial').assign(group=t1_df['group'])\n",
    "thickness_pial_right = t1_df.filter(like='thickness_rh_pial').assign(group=t1_df['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_left = t1_df.filter(regex='volume_lh$').assign(group=t1_df['group'])\n",
    "meancurv_left = t1_df.filter(regex='meancurv_lh$').assign(group=t1_df['group'])\n",
    "thickness_left = t1_df.filter(regex='thickness_lh$').assign(group=t1_df['group'])\n",
    "\n",
    "volume_right = t1_df.filter(regex='volume_rh$').assign(group=t1_df['group'])\n",
    "meancurv_right = t1_df.filter(regex='meancurv$_rh').assign(group=t1_df['group'])\n",
    "thickness_right = t1_df.filter(regex='thickness_rh$').assign(group=t1_df['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aseg_df = t1_df.iloc[:,-59:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_catplot(df, quantiles, column='aseg', hue=None, height=10, aspect=.8):\n",
    "    \n",
    "    quantile_values = df[column].quantile(quantiles)\n",
    "    df['range'] = pd.cut(df[column],\n",
    "                              bins=quantile_values,\n",
    "                              include_lowest=True)\n",
    "\n",
    "    g = sns.catplot(\n",
    "        x=column, y='ROI', kind='box',\n",
    "        hue=hue, \n",
    "        data=df,\n",
    "        height=height,\n",
    "        aspect=aspect,\n",
    "        col='range',\n",
    "        col_wrap=2,\n",
    "        sharex=False,\n",
    "        sharey=False,\n",
    "    )\n",
    "\n",
    "    g.set_titles(\"\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biplot(df_scores, df_loadings, pc1, pc2, ax, num_features=5):\n",
    "\n",
    "    loadings_magnitude = df_loadings.pow(2).sum(axis=0)\n",
    "    top_features = loadings_magnitude.nlargest(num_features).index\n",
    "    df_loadings_limited = df_loadings[top_features]\n",
    "\n",
    "    ax.scatter(df_scores[pc1], df_scores[pc2], color='b', alpha=0.5)\n",
    "    ax.set_xlabel(pc1, fontsize=10)\n",
    "    ax.set_ylabel(pc2, fontsize=10)\n",
    "\n",
    "    ax2 = ax.twinx().twiny()\n",
    "\n",
    "    x_lim, y_lim = ax.get_xlim(), ax.get_ylim()\n",
    "    max_x, max_y = x_lim[1] - x_lim[0], y_lim[1] - y_lim[0]\n",
    "    max_length = max(max_x, max_y) / 2\n",
    "    norm_loadings = df_loadings_limited / df_loadings_limited.abs().max().max() * max_length\n",
    "\n",
    "    for col in norm_loadings.columns:\n",
    "        tipx = norm_loadings.loc[pc1, col]\n",
    "        tipy = norm_loadings.loc[pc2, col]\n",
    "        \n",
    "        ax2.arrow(0, 0, tipx, tipy, color='r')\n",
    "\n",
    "        #if tipx > 0:\n",
    "        ax2.text(tipx, tipy, col, fontdict={'color': 'g', 'weight': 'bold', 'size': 8}, ha='center')\n",
    "        #else:\n",
    "            #ax2.text(tipx, tipy, col, fontdict={'color': 'g', 'weight': 'bold', 'size': 8}, ha='left')\n",
    "\n",
    "    mpl_axes_aligner.align.xaxes(ax, 0, ax2, 0, 0.5)\n",
    "    mpl_axes_aligner.align.yaxes(ax, 0, ax2, 0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_biplots(df_scores: pd.DataFrame, df_loadings: pd.DataFrame, num_plots: int = 6, grid_size=(3, 2)) -> None:\n",
    "\n",
    "    num_components = df_scores.shape[1]\n",
    "    random_pairs = np.random.choice(range(num_components), (num_plots, 2), replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (pc1_idx, pc2_idx) in enumerate(random_pairs):\n",
    "\n",
    "        pc1 = f\"PC{pc1_idx+1}\"\n",
    "        pc2 = f\"PC{pc2_idx+1}\"\n",
    "\n",
    "        biplot(df_scores, df_loadings, pc1, pc2, axes[i])\n",
    "\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_and_clean(df, value_name, parcellation_name, pattern):\n",
    "    \n",
    "    reshaped_df = pd.melt(\n",
    "        df.reset_index(),\n",
    "        id_vars=[\"index\", \"group\"],\n",
    "        var_name=\"ROI\",\n",
    "        value_name=value_name\n",
    "    )\n",
    "\n",
    "    reshaped_df[\"ROI\"] = reshaped_df[\"ROI\"].str.replace(pattern, \"\")\n",
    "    reshaped_df[\"parcellation\"] = parcellation_name\n",
    "    return reshaped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_matrix(df, vmax=None):\n",
    "\n",
    "    corr = df.corr()\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=vmax, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "como nem todos os atributos possuem uma distribuição normal é importante normalizar os atributos para evitar problemas com métodos sensiveis a distribuição.  \n",
    "devido a natureza dos dados os outiliers podem ser casos especificos e que são importantes para a analise e não devem ser removidos. modelos robustos a outliers como random forest e gradient boosting são ideais para esse caso  \n",
    "devido a quantidade de atributos e a não linearidade uma redução com pca pode trazer beneficios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aseg_long = pd.melt(\n",
    "    aseg_df.reset_index(),\n",
    "    id_vars=[\"index\", \"group\"],\n",
    "    var_name=\"ROI\",\n",
    "    value_name=\"aseg\",\n",
    ").drop(columns=[\"index\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "plot_catplot(aseg_long, quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desikan-Killiany com e sem Pial não possue diferença para as métricas de volume e thickness causando redundancia de valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2009s_left = reshape_and_clean(meancurv_a2009s_left, \"meancurv\", \"Destrieux\", \"_meancurv_lh_a2009s\")\n",
    "a2009s_right = reshape_and_clean(meancurv_a2009s_right, \"meancurv\", \"Destrieux\", \"_meancurv_rh_a2009s\")\n",
    "desikan_pial_left = reshape_and_clean(meancurv_pial_left, \"meancurv\", \"Desikan-Killiany with Pial\", \"_meancurv_lh_pial\")\n",
    "desikan_pial_right = reshape_and_clean(meancurv_pial_right, \"meancurv\", \"Desikan-Killiany with Pial\", \"_meancurv_rh_pial\")\n",
    "desikan_left = reshape_and_clean(meancurv_left, \"meancurv\", \"Desikan-Killiany\", \"_meancurv_lh\")\n",
    "desikan_right = reshape_and_clean(meancurv_right, \"meancurv\", \"Desikan-Killiany\", \"_meancurv_rh\")\n",
    "\n",
    "meancurv_df = pd.concat(\n",
    "    [a2009s_left, a2009s_right, desikan_left, desikan_right, desikan_pial_left, desikan_pial_right]\n",
    ").drop(columns=[\"index\"]).reset_index(drop=True)\n",
    "\n",
    "quantiles = [0.0, 0.5, 1.0]\n",
    "plot_catplot(meancurv_df, quantiles, column='meancurv', hue='parcellation', height=25, aspect=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2009s_left = reshape_and_clean(volume_a2009s_left, \"volume\", \"Destrieux\", \"_volume_lh_a2009s\")\n",
    "a2009s_right = reshape_and_clean(volume_a2009s_right, \"volume\", \"Destrieux\", \"_volume_rh_a2009s\")\n",
    "desikan_pial_left = reshape_and_clean(volume_pial_left, \"volume\", \"Desikan-Killiany with Pial\", \"_volume_lh_pial\")\n",
    "desikan_pial_right = reshape_and_clean(volume_pial_right, \"volume\", \"Desikan-Killiany with Pial\", \"_volume_rh_pial\")\n",
    "desikan_left = reshape_and_clean(volume_left, \"volume\", \"Desikan-Killiany\", \"_volume_lh\")\n",
    "desikan_right = reshape_and_clean(volume_right, \"volume\", \"Desikan-Killiany\", \"_volume_rh\")\n",
    "\n",
    "volume_df = pd.concat(\n",
    "    [a2009s_left, a2009s_right, desikan_pial_left, desikan_pial_right, desikan_left, desikan_right]\n",
    ").drop(columns=[\"index\"]).reset_index(drop=True)\n",
    "\n",
    "quantiles = [0.0, 0.5, 1.0]\n",
    "plot_catplot(volume_df, quantiles, column='volume', hue='parcellation', height=25, aspect=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2009s_left = reshape_and_clean(thickness_a2009s_left, \"thickness\", \"Destrieux\", \"_thickness_lh_a2009s\")\n",
    "a2009s_right = reshape_and_clean(thickness_a2009s_right, \"thickness\", \"Destrieux\", \"_thickness_rh_a2009s\")\n",
    "desikan_pial_left = reshape_and_clean(thickness_pial_left, \"thickness\", \"Desikan-Killiany with Pial\", \"_thickness_lh_pial\")\n",
    "desikan_pial_right = reshape_and_clean(thickness_pial_right, \"thickness\", \"Desikan-Killiany with Pial\", \"_thickness_rh_pial\")\n",
    "desikan_left = reshape_and_clean(thickness_left, \"thickness\", \"Desikan-Killiany\", \"_thickness_lh\")\n",
    "desikan_right = reshape_and_clean(thickness_right, \"thickness\", \"Desikan-Killiany\", \"_thickness_rh\")\n",
    "\n",
    "thickness_df = pd.concat(\n",
    "    [a2009s_left, a2009s_right, desikan_pial_left, desikan_pial_right, desikan_left, desikan_right]\n",
    ").drop(columns=[\"index\"]).reset_index(drop=True)\n",
    "\n",
    "quantiles = [0.0, 0.5, 1.0]\n",
    "plot_catplot(thickness_df, quantiles, column='thickness', hue='parcellation', height=25, aspect=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grande parte dos atributos mostram uma interdependencia podendo significar multicolinearidade. isso se deve principalmente por redundancia, por exemplo, total gray matter volume and supratentorial volume estão fortemente correlacionado porque um é um subconjunto ou derivado do outro. neste caso a seleção de atributos se torna importante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = t1_df.filter(like='volume')\n",
    "volume_norm = (volume - volume.mean()) / volume.std()\n",
    "corr_volume = volume_norm.corr()\n",
    "\n",
    "meancurv = t1_df.filter(like='meancurv')\n",
    "meancurv_norm = (meancurv - meancurv.mean()) / meancurv.std()\n",
    "corr_meancurv = meancurv_norm.corr()\n",
    "\n",
    "thickness = t1_df.filter(like='thickness')\n",
    "thickness_norm = (thickness - thickness.mean()) / thickness.std()\n",
    "corr_thickness = thickness_norm.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_matrix(thickness_norm, vmax=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t1_df.drop(columns=['subject', 'group'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "cov = np.cov(X_scaled.T)\n",
    "\n",
    "eigvals, eigvecs = eigh(cov)\n",
    "indices = np.argsort(eigvals)[::-1]\n",
    "eigvals, eigvecs = eigvals[indices], eigvecs[:, indices]\n",
    "\n",
    "explained_variance = eigvals / eigvals.sum()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "threshold = 0.8\n",
    "n_components = np.argmax(cumulative_variance > threshold) + 1\n",
    "\n",
    "pca_df = pd.DataFrame({\n",
    "    'Principal Component': [f'{i+1}' for i in range(len(explained_variance[:n_components]))],\n",
    "    'Explained Variance': explained_variance[:n_components],\n",
    "    'Cumulative Explained Variance': cumulative_variance[:n_components]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Principal Component', y='Explained Variance', data=pca_df, color='lightblue', label='Explained Variance')\n",
    "plt.step(pca_df['Principal Component'], pca_df['Cumulative Explained Variance'], where='mid', color='darkblue', linewidth=2, label='Cumulative Explained Variance')\n",
    "\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "df_scores = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "df_loadings = pd.DataFrame(pca.components_, columns=X.columns, index=[f'PC{i+1}' for i in range(n_components)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_matrix(df_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_biplots(df_scores, df_loadings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logistic regression\n",
    "- support vector machine\n",
    "- random forest\n",
    "- gradient boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
